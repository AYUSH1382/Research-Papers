# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

 

### Introduction

A new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers has been introduced in the paper.

Language model pre-training has been shown to be effective for improving many natural language processing tasks. There are two existing strategies for applying pre-trained language representations to down-stream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pre-trained parameters. Both these approaches use unidirectional language models to learn general language representations. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.

 In this paper, they have worked on improving the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers using a “masked language model” (MLM). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. BERT uses masked language models to enable pre-trained deep bidirectional representations. BERT is the first fine-tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.

 There are different pre-training general language representations approaches. In Unsupervised Feature-based Approaches the Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch. These approaches have been generalized to coarser granularities, such as sentence embeddings or paragraph embeddings. ELMo and its predecessor extract context-sensitive features from a left-to-right and a right-to-left language model. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks including question answering, sentiment analysis, and named entity recognition. In Unsupervised Fine-tuning Approaches sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task. At least partly due to this advantage, OpenAI GPT achieved previously state-of-the-art results on many sentence level tasks from the GLUE benchmark. In Transfer Learning from Supervised Data work showing effective transfer from supervised tasks with large datasets, such as natural language inference and machine translation.

### BERT

There are two steps in the framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. BERT’s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation in the tensor2tensor library. Primarily they report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M). To make BERT handle a variety of down-stream tasks, the input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g. Question, Answer) in one token sequence. The first token of every sequence is always a special classification token ([CLS]). They try differentiating the sentences in two ways. First, they separate them with a special token ([SEP]). Second, they add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. They pre-train BERT using two unsupervised tasks Masked LM and Next Sentence Prediction (NSP). In order to train a deep bidirectional representation, they mask some percentage of the input tokens at random, and then predict those masked tokens. These procedure is known as“masked LM” (MLM). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output SoftMax over the vocabulary, as in a standard LM. Although it obtains a bidirectional pre-trained model, a downside is that it’s creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning. 

 In order to train a model that understands sentence relationships, they have pre-trained for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A, and 50% of the time it is a random sentence from the corpus. For the pre-training corpus they used the Books-Corpus (800M words) and English Wikipedia (2,500M words). 

 Fine-tuning is straightforward since the self attention mechanism in the Transformer allows BERT to model many downstream tasks— whether they involve single text or text pairs—by swapping out the appropriate inputs and outputs. For each task, they plug in the task specific inputs and outputs into BERT and finetune all the parameters end-to-end. Compared to pre-training, fine-tuning is relatively inexpensive.  

### Experiments

They have experimented the BERT fine-tuning on 11 NLP tasks. The General Language Understanding Evaluation (GLUE) is a collection of diverse natural language understanding tasks. They compute a standard classification loss with C and W, i.e., log(SoftMax(CWT ) where C is final hidden vector and W is the classification layer weights. They have use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. Both BERTBASE and BERTLARGE outperform all systems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy improvement over the prior state of the art. It is also observed that BERTLARGE significantly outperforms BERTBASE across all tasks, especially those with very little training data. 

 The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowdsourced question/answer pair. Given a question and a passage from Wikipedia containing the answer, the task is to predict the answer text span in the passage. The training objective is the sum of the log-likelihoods of the correct start and end positions. They used modest data augmentation in the system by first fine-tuning on TriviaQA before fine-tuning on SQuAD. The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph, making the problem more realistic. They did not use TriviaQA data for this model. The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference. When fine-tuning on the SWAG dataset, they construct four input sequences, each containing the concatenation of the given sentence (sentence A) and a possible continuation (sentence B).

###  Ablation Studies

They have performed ablation experiments over a number of facets of BERT in order to better understand their relative importance. They have demonstrated the importance of the deep bidirectionality of BERT by evaluating two pre-training objectives using exactly the same pre-training data, fine-tuning scheme, and hyperparameters as BERTBASE.

The two pre-training objectives are: a)**No NSP**: A bidirectional model which is trained using the “masked LM” (MLM) but without the “next sentence prediction” (NSP) task. b)LTR & No NSP: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. Removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD. The BiLSTM hurts performance on the GLUE tasks. 

It is recognized that it would be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models, as ELMo does. However this has proved to be twice as expensive as a single bidirectional model; this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.

 They have explored the effect of model size on fine-tuning task accuracy. It is seen that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks. The feature-based approach, where fixed features are extracted from the pre-trained model, has certain advantages. They compared the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task. They have use the representation of the first sub-token as the input to the token-level classifier over the NER label set. To ablate the fine-tuning approach, they applied the feature-based approach by extracting the activation from one or more layers without fine-tuning any parameters of BERT.

BERTLARGE performs competitively with state-of-the-art methods. It shows that BERT is effective for both fine-tuning and feature-based approaches.

###  Conclusion

Lately it is seen that unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to benefit from deep unidirectional architectures. This model has contributed in generalizing the findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP task.

